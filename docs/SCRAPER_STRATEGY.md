# Web Scraper Strategy: 2-Minute Mini Cycles
## Integrated with Dataflow & Contracts Review

**Last Updated:** 2025-10-29  
**Status:** Strategy implementation in progress

---

## Current Problems

### ‚ö†Ô∏è **Performance Issues**
1. **Cycles too long**: 7+ minutes (target: 2 minutes max)
2. **Sequential processing**: One institution at a time
3. **Redundant validation**: Validating every URL even if already known
4. **No change detection**: Always re-scraping instead of checking for changes
5. **Discovery mixed with scraping**: Same cycle does both (slow)

### üìä **Current State Analysis**

**Discovery:**
- FFG: 207 URLs discovered, 206 unscraped ‚úÖ (Discovery works!)
- WKO: 158 URLs discovered, 156 unscraped ‚úÖ (Discovery works!)
- AMS: 74 URLs discovered ‚úÖ
- Total unscraped: ~450+ URLs waiting

**Scraping:**
- Only 59 programs scraped (6% of potential)
- Processing 50-200 URLs per institution (too many in one cycle)
- Each URL takes ~12-15 seconds (validation + scraping)

**Requirements:**
- co_financing: 0% (needs enhancement)
- trl_level: 3% (needs enhancement)
- 26/59 programs have 5+ categories (44%)

---

## Proposed Strategy: Separate Phases

### üéØ **Core Principle: Separation of Concerns**

**Discovery Phase** (Fast, 2 min max):
- Quick URL discovery only
- No scraping, just find URLs
- Can run in parallel across institutions

**Scraping Phase** (Focused, 2-5 min):
- Process discovered URLs in batches
- Only scrape actual program pages
- Skip query/filter pages

**Update Phase** (Periodic, 5-10 min):
- Check existing programs for changes
- Re-scrape if changed
- Update requirements if improved

**Enhancement Phase** (Background):
- Screen existing requirements for gaps
- Improve extraction patterns
- Find new websites/institutions

---

## Phase 1: Mini Discovery Cycles (2 min max)

### Goal: Find new URLs quickly

**Strategy:**
- Process 1-3 institutions per cycle (30-60 seconds each)
- Only discovery, NO scraping
- Fast loading (`domcontentloaded` only)
- Store URLs for later scraping
- Skip validation entirely

**Implementation:**
```typescript
async discoverUrlsOnly(institution, maxTime = 120000) {
  // Quick discovery: 30-60s per institution
  // Extract links, store to discovery-state.json
  // Return count of new URLs found
  // NO scraping, NO validation
}
```

**Schedule:**
- Run every 30 minutes
- Rotate through all institutions
- Track which institutions need more discovery

**Output:**
- New URLs added to `unscrapedUrls` array
- Ready for scraping phase

---

## Phase 2: Focused Scraping Cycles (2-5 min)

### Goal: Scrape programs from discovered URLs

**Strategy:**
- Process 20-50 URLs per cycle (previously discovered)
- Skip query URLs (already explored in discovery)
- Only scrape detail pages
- Validate quickly (fast checks only)

**Prioritization:**
1. **High-potential institutions first** (FFG, WKO, AMS)
2. **Oldest unscraped URLs first** (FIFO)
3. **Batch by institution** (process all FFG URLs together)

**Implementation:**
```typescript
async scrapeDiscoveredUrls(limit = 50, timeout = 300000) {
  // Load unscraped URLs from discovery-state
  // Process top N URLs
  // Skip query URLs (already explored)
  // Only scrape detail pages
  // Update discovery state after scraping
}
```

**Schedule:**
- Run every 2-3 hours
- Process 50-100 URLs per run
- Focus on institutions with most unscraped URLs

---

## Phase 3: Change Detection (Periodic)

### Goal: Keep existing programs up-to-date

**Strategy:**
- Check existing programs for changes
- Lightweight check first (status, title, basic fields)
- Full re-scrape only if changed

**Change Detection:**
1. **Quick check** (10-15 seconds):
   - HTTP HEAD request for status
   - Hash page content (basic)
   - Compare with stored hash
   
2. **Deep check** (if changed, 30-60 seconds):
   - Full scrape of changed programs
   - Compare requirements
   - Update if different

**Implementation:**
```typescript
async checkProgramChanges(maxPrograms = 50, timeout = 300000) {
  // Load existing programs
  // Quick check: status + content hash
  // Re-scrape only changed programs
  // Update if requirements changed
}
```

**Schedule:**
- Run weekly (full check)
- Or run monthly (lightweight check)
- Focus on recently scraped programs first

---

## Phase 4: Requirement Enhancement (Background)

### Goal: Fill gaps in existing programs

**Strategy:**
- Identify programs with missing categories
- Re-scrape only sections with missing data
- Improve extraction patterns based on gaps

**Gaps to Fill:**
- co_financing: 0% ‚Üí Target 30%+
- trl_level: 3% ‚Üí Target 20%+
- compliance: 7% ‚Üí Target 25%+
- revenue_model: 17% ‚Üí Target 30%+

**Implementation:**
```typescript
async enhanceProgramRequirements() {
  // Find programs missing key categories
  // Re-scrape eligibility/requirements sections
  // Apply improved extraction patterns
  // Update only changed requirements
}
```

**Schedule:**
- Run monthly
- Focus on high-value programs first
- Track improvement metrics

---

## Phase 5: New Website Discovery (Continuous)

### Goal: Find new institutions/websites

**Strategy:**
- Search for funding programs online
- Analyze competitor databases
- User suggestions
- Institutional directories

**Sources:**
- Google search: "[country] funding programs"
- Government directories
- EU funding databases
- Regional development agencies

**Implementation:**
```typescript
async discoverNewInstitutions() {
  // Search for new funding sources
  // Validate institutional websites
  // Add to institutionConfig.ts
  // Start discovery phase for new institutions
}
```

**Schedule:**
- Run monthly or quarterly
- Manual review before adding
- Test with discovery cycle first

---

## Proposed Schedule

### **Daily:**
- **Discovery cycles** (every 30 min): 2 min √ó 48 = ~96 min/day
  - Finds new URLs continuously
  - Keeps discovery state fresh
  
- **Scraping cycles** (every 2-3 hours): 5 min √ó 8 = ~40 min/day
  - Processes discovered URLs
  - Adds new programs

### **Weekly:**
- **Change detection** (once): 10-15 min
  - Checks existing programs for updates
  - Updates changed programs

- **Requirement enhancement** (once): 15-30 min
  - Fills gaps in existing programs
  - Improves data quality

### **Monthly:**
- **New website discovery** (once): 30-60 min
  - Finds new institutions
  - Expands coverage

---

## Technical Implementation

### **1. Separate Discovery Service**

```typescript
class DiscoveryService {
  async quickDiscovery(institution, timeout = 60000) {
    // Fast URL extraction only
    // No validation, no scraping
    // Returns new URLs found
  }
}
```

### **2. Separate Scraping Service**

```typescript
class ScrapingService {
  async scrapeBatch(urls, timeout = 300000) {
    // Process URLs from discovery-state
    // Skip query URLs
    // Only detail pages
  }
}
```

### **3. Change Detection Service**

```typescript
class ChangeDetectionService {
  async checkChanges(programs) {
    // Quick checks first
    // Full scrape if changed
  }
}
```

### **4. State Management**

```json
{
  "discovery": {
    "lastRun": "2025-10-29T12:00:00Z",
    "institutionsProcessed": ["FFG", "WKO"],
    "newUrlsFound": 15
  },
  "scraping": {
    "lastRun": "2025-10-29T11:00:00Z",
    "urlsProcessed": 50,
    "programsAdded": 5
  },
  "changes": {
    "lastRun": "2025-10-28T00:00:00Z",
    "programsChecked": 59,
    "programsChanged": 2
  }
}
```

---

## Benefits of This Approach

### ‚úÖ **Speed**
- Discovery cycles: 2 min (vs 7+ min currently)
- Scraping cycles: 2-5 min (focused, no discovery overhead)
- Change detection: 10-15 min (vs full re-scrape)

### ‚úÖ **Efficiency**
- Discovery in parallel (can run multiple institutions)
- Scraping focused on high-value URLs
- No redundant validation

### ‚úÖ **Scalability**
- Discovery can find 1000s of URLs quickly
- Scraping processes in manageable batches
- Change detection prevents stale data

### ‚úÖ **Flexibility**
- Can pause/resume any phase independently
- Can prioritize high-value institutions
- Can adapt schedule based on results

---

## Migration Path

### **Phase 1: Separate Discovery (Week 1)**
- Extract discovery logic to separate service
- Create fast discovery cycles (2 min max)
- Test with FFG, WKO, AMS

### **Phase 2: Optimize Scraping (Week 2)**
- Focus scraping on discovered URLs only
- Remove discovery from scraping cycles
- Process in batches (50-100 URLs)

### **Phase 3: Add Change Detection (Week 3)**
- Implement lightweight change detection
- Re-scrape only changed programs
- Track change frequency

### **Phase 4: Requirement Enhancement (Week 4)**
- Identify gaps in existing programs
- Re-scrape missing sections
- Improve extraction patterns

---

## Metrics to Track

### **Discovery Metrics:**
- URLs discovered per cycle
- Time per institution
- Success rate (valid URLs found)

### **Scraping Metrics:**
- Programs scraped per cycle
- Time per program
- Success rate (valid programs extracted)

### **Change Detection Metrics:**
- Programs checked
- Programs changed
- Update frequency

### **Quality Metrics:**
- Requirements coverage (%)
- Category population rates
- Data completeness score

---

## Quick Wins (Can Implement Today)

1. **Separate discovery from scraping** ‚úÖ
   - Create `discoverUrlsOnly()` method
   - Fast cycles (domcontentloaded only)
   - Store URLs, scrape later

2. **Skip query URL scraping** ‚úÖ
   - Already implemented
   - Query URLs are listing pages only
   - Explore in discovery, skip in scraping

3. **Batch processing** ‚úÖ
   - Process 20-50 URLs per cycle
   - Focus on one institution at a time
   - Track progress

4. **Parallel discovery** üîÑ
   - Can run discovery for multiple institutions simultaneously
   - Use Promise.all() for 2-3 institutions
   - Significantly faster

---

## Next Steps

1. ‚úÖ **Analyze current results** - Review what we have
2. ‚úÖ **Enhance transformation logic** - Handle all 18 categories, normalize fields
3. ‚úÖ **Add completeness scoring** - API returns quality scores
4. ‚úÖ **Add freshness detection** - Track stale data
5. ‚è≥ **Implement discovery-only cycles** - Test 2-minute cycles (Next)
6. ‚è≥ **Optimize scraping batches** - Focus on high-value URLs (Next)
7. ‚è≥ **Add change detection** - Prevent stale data (Next)

---

## ‚úÖ Completed: Contract Review Integration

### **Core Improvements (Based on Dataflow Review)**

1. **Centralized Transformation** ‚úÖ
   - `transformEligibilityToCategorized()` enhanced in `pages/api/programs.ts`
   - Now handles ALL 18 requirement categories
   - Exported for use across scraper, API, engines
   - Handles field name variations (co_financing vs cofinancing_pct)

2. **Field Normalization** ‚úÖ
   - Scraper extracts `cofinancing_pct` from categorized requirements
   - Standardized field names across all data paths
   - Consistent Program interface

3. **Completeness Scoring** ‚úÖ
   - `calculateCompletenessScore()` added to API
   - Scores programs 0-100 based on filled fields
   - Weighted by importance (requirements > contacts)

4. **Freshness Detection** ‚úÖ
   - `isProgramFresh()` checks if data scraped within 24 hours
   - Used in API responses
   - Helps UI show warnings for stale data

5. **Institution IDs** ‚úÖ
   - Interface updated to support unique IDs (optional for backward compatibility)
   - Can add IDs incrementally

### **Files Updated**
- ‚úÖ `pages/api/programs.ts` - Enhanced transformation, scoring, freshness
- ‚úÖ `src/lib/webScraperService.ts` - Normalized field extraction
- ‚úÖ `src/lib/institutionConfig.ts` - Added ID support (optional)
- ‚úÖ `docs/STRATEGY_IMPLEMENTATION.md` - Implementation tracking

---

---

## ‚úÖ IMPLEMENTED SOLUTIONS (Current Status)

### **1. Discovery Separates Category from Detail Pages** ‚úÖ **CRITICAL FIX**
- **Location**: `src/lib/webScraperService.ts` line 451-463, 335-351
- **Logic**: Filters category/listing pages - only detail pages go to `unscrapedUrls`
- **Result**: Discovery finds actual scrapable programs, not just category pages

### **2. Enhanced Link Extraction from Category Pages** ‚úÖ
- **Location**: `src/lib/webScraperService.ts` line 1912-1979
- **Logic**: Multiple strategies to find program links in category pages:
  - Pattern matching (node IDs, program codes, calls)
  - Parent element analysis (cards/articles with program text)
  - Funding amount/deadline detection in context
  - Table/list structure detection
- **Result**: Finds more program detail links from listing pages

### **3. Batch Processing (15 URLs per institution in cycle mode)** ‚úÖ
- **Location**: `src/lib/webScraperService.ts` line 155
- **Change**: `15 URLs per institution` in cycle mode, `50` in full mode
- **Result**: Processes all 32 institutions efficiently

### **4. Change Detection** ‚úÖ
- **Location**: `src/lib/webScraperService.ts` line 170-181
- **Logic**: Skips URLs scraped in last 24 hours
- **Result**: No redundant re-scraping of recent data

### **5. Uses Unscraped URLs** ‚úÖ
- **Location**: `src/lib/webScraperService.ts` line 136-149
- **Logic**: Loads existing `unscrapedUrls` from discovery-state.json FIRST
- **Result**: Processes discovered detail pages, not category pages

### **6. Quick Validation** ‚úÖ
- **Location**: `src/lib/webScraperService.ts` line 197-202
- **Logic**: Uses `isProgramDetailPage()` and `preValidateUrl()` for detail pages
- **Result**: Faster validation, skips category/query URLs

---

## üöÄ HOW TO RUN - TWO-PHASE APPROACH (RECOMMENDED)

### **Phase 1: Discovery Only** (Run First)
```bash
node scripts/run-discovery-only.js incremental
```
- **Time**: ~3-5 minutes (all 32 institutions, parallel batches)
- **What it does**: 
  1. Finds program detail page URLs from category/listing pages
  2. Filters out category pages (only detail pages to `unscrapedUrls`)
  3. Saves to `discovery-state.json`
  4. Shows stats: new detail pages found per institution

### **Phase 2: Scraping** (Run After Discovery)
```bash
node scripts/run-scraper-direct.js cycle incremental
```
- **Time**: ~5-10 minutes (15 URLs per institution)
- **What it does**: 
  1. Loads detail URLs from `discovery-state.json`
  2. Skips category/listing pages (already filtered in discovery)
  3. Scrapes actual program data with 18 categories
  4. Saves to `scraped-programs-latest.json`

### **Alternative: Combined Mode** (Discovery + Scraping in one run)
```bash
node scripts/run-scraper-direct.js cycle incremental
```
- Does discovery + scraping in same run
- Slower but convenience option

### **Legacy API Method** (Requires Next.js server)
```bash
node scripts/trigger-full-scraper.js cycle incremental
```
- Requires `npm run dev` to be running
- Uses `/api/scraper` endpoint

---

## ‚è±Ô∏è TIME ESTIMATES

### **Per Run:**
- **Discovery + Scraping**: 3-5 minutes (30 URLs)
- **Discovery Only**: 3-5 minutes (all institutions)

### **For All 450 URLs:**
- Run scraper **15 times** = 45-75 minutes total
- Each run: ~20-30 new programs
- **Incremental progress**: No long waits

### **Strategy Goal vs Reality:**
- **Goal**: 2-5 min batches ‚úÖ (achieved with 30 URL batches)
- **Goal**: Change detection ‚úÖ (skips < 24h)
- **Goal**: Uses discovered URLs ‚úÖ (fixed)

---

## ‚ùå NOT IMPLEMENTED (Low Priority)

1. **Separate Discovery Service** - Not needed (discovery works fine)
2. **Separate Scraping Service** - Not needed (same method works)
3. **Content hash change detection** - Current time-based is sufficient
4. **Change detection service** - Current inline check is sufficient

---

## üìä CURRENT STATUS (Updated 2025-10-29)

- **527 URLs discovered** ‚úÖ (527 known URLs across all institutions)
- **394 scrapable detail pages** ‚úÖ (100% scrapable - all detail pages, 0 category pages)
- **80 programs currently scraped** (19% of scrapable pages)
- **Scraper uses them** ‚úÖ (fixed - loads from discovery-state.json)
- **Fast batches (15 URLs per institution)** ‚úÖ (cycle mode processes all institutions)
- **Change detection** ‚úÖ (skips < 24h)
- **Discovery filtering** ‚úÖ (filters category pages from unscrapedUrls)

### Discovery Results (Latest Run):
- **435 total scrapable detail pages** (after cleanup)
- **+336 new detail pages found** in latest discovery run
- **Top performers**: BMK (223), FWF (91), KfW (21)
- **Speed**: 49.3 detail pages/minute

### Architecture Overview:
```
institutionConfig.ts (Static)
    ‚Üì (defines institutions, seed URLs, funding types)
webScraperService.ts (Dynamic Discovery + Scraping)
    ‚Üì (discovers URLs ‚Üí saves to)
discovery-state.json (Dynamic State)
    ‚Üì (unscrapedUrls ‚Üí used by scraper)
scraped-programs-latest.json (Scraped Data Output)
```

### How It Works:
1. **InstitutionConfig.ts**: Static config - defines 32 institutions, their seed URLs, funding types
2. **Discovery**: Dynamically explores category pages ‚Üí finds program detail links ‚Üí saves to `discovery-state.json`
3. **Discovery-state.json**: Dynamic state per institution (knownUrls, unscrapedUrls, exploredSections)
4. **Scraper**: Loads unscrapedUrls from state ‚Üí scrapes detail pages ‚Üí saves to `scraped-programs-latest.json`
5. **NOT continuous**: Runs on-demand (manual trigger or scheduled). Each run processes batches incrementally.

### Scalability (2500+ Programs):
**Current**: 394 scrapable URLs, 80 scraped (19%)
**Target**: 2500 programs
**Approach**:
- Discovery can find 2500+ URLs (already scalable - found 435 in one run)
- Scraping in cycles: 15 URLs per institution = 480 URLs per cycle (all 32 institutions)
- **For 2500 programs**: ~5-6 scraper cycles needed (each 5-10 min = 30-60 min total)
- **Not continuous parsing**: Runs in batches when triggered, processes incrementally
- **State-based**: Tracks progress in discovery-state.json, never re-scrapes without change detection

